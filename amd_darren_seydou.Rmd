---
title: "Projet ADM - MIASHS"
author: "Seydou NIARE, Darren SAMRETH"
output:
  html_document: default
  html_notebook: default
  word_document: default
---

## I.Préparation des données et enrichissement

### 1. Importation des libraries


```{r, message=FALSE, warning=FALSE}
library(FactoMineR)
library(factoextra)
library(tidyr)
library(pls)

library(MASS)
library(scales)
```

### 2. Importation des données

Les fichiers CSV du dossier data sont tous importés dans un dataframe nommé df.

Deux autres dataframe sont crées pour répartir les données train et les données test.

Les dates et les modalités sans valeurs sont supprimées, ainsi que les lignes qui contiennent des données manquantes. 
```{r}
files = list.files(path = "data/", pattern="*.csv")
for (i in 1:length(files)){
  files[i] = paste("data/",files[i],sep="")
}
df = do.call(rbind, lapply(files, function(x) read.csv2(x, sep = ";", stringsAsFactors = F)))

files = list.files(path = "data/", pattern="*-train.csv")
for (i in 1:length(files)){
  files[i] = paste("data/",files[i],sep="")
}
df_train = do.call(rbind, lapply(files, function(x) read.csv2(x, sep = ";", stringsAsFactors = F)))

files = list.files(path = "data/", pattern="*-test.csv")
for (i in 1:length(files)){
  files[i] = paste("data/",files[i],sep="")
}
df_test = do.call(rbind, lapply(files, function(x) read.csv2(x, sep = ";", stringsAsFactors = F)))

df = na.exclude(df[,1:29])[,3:29]
df_train = na.exclude(df_train[,1:29])[,3:29]
df_test = na.exclude(df_test[,1:29])[,3:29]
```


## II. Analyse descriptive par ACP

Commençons par effectuer une ACP en utilisant toutes les données quantitatives à notre disposition. Nous travaillons avec des données standardisées afin de prévenir tout problème d'unité de mesure.

```{r}
res_pca <- PCA(df, scale.unit = T, graph=F)
```

**Commençons par déterminer un nombre pertinant de composantes à retenir.**

```{r}
fviz_screeplot(res_pca, ncp=10)
```

Deux composantes semble être suffisant, étant donné que l'ajout d'une troisième dimension ne semble pas
ajouter une valeur-ajoutée assez conséquente pour être indispensable.

**Vérifions la contribution de chaque dimenension par rapport aux variables.**


```{r}
barplot(res_pca$var$contrib[,1])
barplot(res_pca$var$contrib[,2])
```

En étudiant les corrélations apportées par chaque dimension, on s'aperçoit que la première explique une grande partie des données. Les deux autres ne font qu'apporter plus d'information sur deux variables non corrélées dans la première dimension.

```{r}
fviz_pca_var(res_pca, axes=c(1,2))
```

Ce graphe des variables confirme qu'une grande majorité des variables sont bien représentés par les deux dimensions.

On peut déjà remarquer que trois variables (sigmap, sigmam et skew) sont inversement corrélées avec la plupart des autres variables sur l'axe de la première dimension.

**Etudions maitenant les individus :**

```{r}
fviz_pca_ind(res_pca)
```

Un groupe d'individu semble former une aggrégation dans les valeurs négative de la dimension 1, alors que les individus des valeurs positives semblent plus dispersés.

## III. Modélisation de la prédiction de h13d par modèles linéaires (classique, pcr et pls)

Tout d'abord, projetons les données d'apprentisage et de test sur les axes de l'ACP.

```{r}
res_pca = PCA(df_train, scale.unit = T, graph = F)
train_proj = res_pca$ind$coord
test_proj = t(apply(df_test, MARGIN = 1, FUN = function(x) { (x - res_pca$call$centre)/res_pca$call$ecart.type} )) %*% res_pca$svd$V
train_proj <- as.data.frame(train_proj)
test_proj <- as.data.frame(test_proj)
test_proj$h13d <- df_test$h13d
train_proj$h13d <- df_train[1:25,]$h13d
colnames(test_proj) <- colnames(train_proj)
```

**Maintenant, comparons différents trois modèles issus de méthodes différentes.**

### 1. Régression classique

```{r, warning=FALSE}
reg = lm(h13d~.,data = df_train)
summary(reg)
```
La régréssion classique ne peut pas fonctionner, car il y a plus de régresseurs que de données dans le jeu d'apprentisssage. 

Nous pourrions supprimer des régresseurs (au moins trois pour pouvoir effectuer la régression), mais nous choisissons plutôt de **travailler directement sur les composantes.**

### 2. Régression sur Composantes Principales

Nous utilisons les données projetées précedemment, et nous effectuons une régression en uilisant les composantes.

```{r}
model_pcr <- lm(h13d ~ ., data = train_proj)
step(model_pcr, direction = "both")
summary(model_pcr)
predict_pcr <- predict(model_pcr, test_proj)
rmse <- sqrt(mean((model_pcr$residuals)^2))
cat('RMSE :',rmse)
```

**Comparons ce modèle avec la méthode Partial Least Square Regression.**

### 3. Partial Least Square Regression

```{r}
plsr_fit <- plsr(h13d ~ ., data = df_train)
summary(plsr_fit)
```

La variance expliquée atteint assez rapidement les 98 % pour cinq composants. Cela doit être un nombre de componsant intéressant.

**Etudions maintenant l'évolution du RMSEP (Root Mean Square Error of Prediction).**
```{r}
plot(RMSEP(plsr_fit))
```

Le choix des cinq composants semble encore être une bonne solution, car il correspond au moment où l'évolution du RMSEP se stabilise. Le modèle est donc quasiment à sa précision optimale. Nous utiliserons donc cinq composants.

```{r}
plot(plsr_fit, ncomp = 5, asp = 1, line = T)
```

** Vérifions maitenant si notre modèle est bon en le testant sur nos données de test.**
```{r}
pred <- predict(plsr_fit, ncomp = 5, newdata = df_test)
rmse <- sqrt(mean((pred - df_test$h13d)^2))
cat('RMSE :',rmse)
```

### 4. Conclusion

La PCR semble être le modèle qui est le plus fiable. C'est donc celui-là que nous choisirions.

## IV. Classification et prédiction des classes d’individus

Construisons une classe "type de vague", qui caractérise les grandes, les moyennes et le petites vagues.

```{r}
df_train$vague <- with(df, ifelse(
  df_train$h13d > 2, 'grande', ifelse(
  df_train$h13d >= 1, 'moyenne','petite')))

df_test$vague <- with(df, ifelse(
  df_test$h13d > 2, 'grande', ifelse(
  df_test$h13d >= 1, 'moyenne','petite')))
```

Maintenant construisons notre classificateur. Il nous permettra de déterminer le type de vague, si nous n'avons accès à aucune information sur la hauteur de la vague.

```{r}
lda_fit <- lda(vague ~., data =df_train[6:28])
plda <- predict(object = lda_fit, newdata = df_train)
prop_lda = lda_fit$svd^2 / sum(lda_fit$svd)

dataset = data.frame(vague = df_train[,"vague"], lda = plda$x)

# plot
library(scales)
p1 <- ggplot(dataset) + geom_point(aes(lda.LD1, lda.LD2, colour = vague, shape = vague), size = 2.5) + 
  labs(x = paste("LD1 (", percent(prop_lda[1]), ")", sep=""),
       y = paste("LD2 (", percent(prop_lda[2]), ")", sep="")) +
  theme_bw() +
  labs(title = "LDA", subtitle = "dataset")

p1
```

**Vérifions l'efficacité de notre classificateur.**

```{r}
pred <- predict(lda_fit, df_test)
cat("Fréquence de bon résultat : ",mean(pred$class == df_test$vague))
```

Ainsi, le classificateur que nous avons crée est bon une fois sur trois, ce qui est quand même intéressant en prenant en compte le fait que nous avons suprimé toutes les données directement en rapport avec la hauteur de la vague elle-même.

## V Utilisation de la classification hiérarchique (bonus)

**FactorMineR::HCPC par defaut:

HCPC proposer 3 cluster pour notre composant principal on a: 
```{r}
  hc<-FactoMineR::HCPC(res_pca,nb.clust = 3)
```

Remarque: Un groupe d'individu semble former une aggrégation dans les valeurs négative de la dimension 1, un groupe d'individu de valeurs positives dispersés et un groupe d'individu entre les deux en rouge dimenseion 2.
